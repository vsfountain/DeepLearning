{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T05:02:23.906798Z",
     "start_time": "2017-10-21T05:02:17.045810Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.data import Dataset, Iterator\n",
    "\n",
    "import glob\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T04:44:33.892859Z",
     "start_time": "2017-10-21T04:44:33.887855Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"np.random.permutation\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T05:39:12.484917Z",
     "start_time": "2017-10-21T05:39:12.462895Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Auxiliary functions for loading data from a folder and obtaining the labels from the file names\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    \n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "        \n",
    "    return id2label, label2id\n",
    "\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    \n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)\n",
    "        \n",
    "        \n",
    "def get_labels(files, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    \n",
    "    for f in files:\n",
    "        y.append(get_label(f, label2id))\n",
    "        \n",
    "    return np.array(y)\n",
    "\n",
    "\n",
    "def input_parser(img_path, label):\n",
    "    \"\"\"\n",
    "    Returns an image file converted into a 3D array and its one hot encoded label\n",
    "    \"\"\"\n",
    "    one_hot = tf.one_hot(label, num_classes)\n",
    "    img_file = tf.read_file(img_path)\n",
    "    img_decoded = tf.image.decode_image(img_file, channels=3)\n",
    "    \n",
    "    return img_decoded, one_hot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T05:41:41.378605Z",
     "start_time": "2017-10-21T05:41:41.367594Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data(root_folder, data_folder, labels, shuffle=False):\n",
    "    \"\"\"\n",
    "    Returns the data split into separate training and validation sets\n",
    "    :param root_folder: name of root folder containing the dataset\n",
    "    :param data_folder: name of the subfolder within the root folder that contains the training or test dataset\n",
    "    :param labels: name of the text file that contains the mapping of class names to integers\n",
    "    :param shuffle: whether or not to shuffle the dataset before performing the training/validation split\n",
    "    \"\"\"\n",
    "    data_location = root_folder + data_folder\n",
    "    label_map = get_label_mapping(root_folder + labels)[1]\n",
    "    data = [file for file in glob.glob(data_location + '*/*')]\n",
    "    labels = get_labels(data, label_map)\n",
    "\n",
    "    tmp = np.vstack((np.asarray(data), np.asarray(labels)))\n",
    "    tmp = tmp.T\n",
    "\n",
    "    if shuffle == True:\n",
    "        np.random.shuffle(tmp)\n",
    "\n",
    "#Perform a 90/10 training/validation split on the dataset\n",
    "        \n",
    "    split_point = -int(0.1*tmp.shape[0])\n",
    "    X_train = tmp[:split_point, 0]\n",
    "    y_train = tmp[:split_point, 1]\n",
    "    X_val = tmp[split_point:, 0]\n",
    "    y_val = tmp[split_point:, 1]\n",
    "    \n",
    "    return X_train, y_train.astype(int), X_val, y_val.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T06:43:31.207078Z",
     "start_time": "2017-10-21T06:43:31.192064Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_to_model(X_train, y_train, X_val, y_val, num_classes, batch_size=100):\n",
    "    num_classes = num_classes\n",
    "    \n",
    "    tf_train = tf.contrib.data.Dataset.from_tensor_slices((tf.constant(X_train),\n",
    "                                                         tf.constant(y_train)))\n",
    "    tf_train = tf_train.map(input_parser, num_threads=4)\n",
    "    tf_val = tf.contrib.data.Dataset.from_tensor_slices((tf.constant(X_val),\n",
    "                                                         tf.constant(y_val)))\n",
    "    tf_val = tf_val.map(input_parser, num_threads=4)\n",
    "\n",
    "    tf_train = tf_train.batch(batch_size)\n",
    "    tf_val = tf_val.batch(batch_size)\n",
    "    iterator = Iterator.from_structure(tf_train.output_types,\n",
    "                                       tf_train.output_shapes)\n",
    "\n",
    "    next_batch = iterator.get_next()\n",
    "    training_init_op = iterator.make_initializer(tf_train)\n",
    "    validation_init_op = iterator.make_initializer(tf_val)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(training_init_op)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                elem = sess.run(next_batch)\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T06:35:37.435517Z",
     "start_time": "2017-10-21T06:35:37.094223Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000,) (45000,) (5000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "root_folder  = 'cifar10-hw1/'\n",
    "train_folder = 'train'\n",
    "labels = 'labels.txt'\n",
    "num_classes = len(get_label_mapping(root_folder + labels)[0])\n",
    "\n",
    "X_train, y_train, X_val, y_val = load_data('cifar10-hw1/', 'train', 'labels.txt', True)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T06:43:49.677368Z",
     "start_time": "2017-10-21T06:43:35.321084Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorShape(None), TensorShape([Dimension(None), Dimension(10)]))\n",
      "(TensorShape(None), TensorShape([Dimension(None), Dimension(10)]))\n"
     ]
    }
   ],
   "source": [
    "load_to_model(X_train, y_train, X_val, y_val, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T07:26:32.767190Z",
     "start_time": "2017-10-20T07:26:32.761184Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T07:30:36.641435Z",
     "start_time": "2017-10-20T07:30:36.612407Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def part1_model(features, labels, mode):\n",
    "    input_layer = tf.reshape(features['x'], [-1, 32, 32, 3])\n",
    "    conv1 = tf.layers.conv2d(inputs=input_layer,\n",
    "                                 filters=32,\n",
    "                                 kernel_size=[5, 5],\n",
    "                                 padding='same',\n",
    "                                 activation=tf.nn.relu)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                   pool_size=[2, 2],\n",
    "                                   strides=2)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(inputs=pool1,\n",
    "                            filters=64,\n",
    "                            kernel_size=[5, 5],\n",
    "                            padding='same',\n",
    "                            activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2,\n",
    "                                   pool_size=[2, 2],\n",
    "                                   strides=2)\n",
    "    \n",
    "    pool2_flat = tf.reshape(pool2, [-1, 8*8*64])\n",
    "    \n",
    "    dense = tf.layers.dense(input=pool2,\n",
    "                            units=1024,\n",
    "                            activation=tf.nn.relu)\n",
    "    \n",
    "    output_layer = tf.layers.dense(input=dense, units=10)\n",
    "    \n",
    "    predictions = {'classes': tf.argmax(input=output_layer, axis=1),\n",
    "                   'prob': tf.nn.softmax(output_layer, name='softmax')}\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    OH_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=OH_labels,\n",
    "                                           logits=output_layer)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = GradientDescentOptimizer(learning_rate=.0001)\n",
    "        train_op = optimizer.minimize(loss=loss,\n",
    "                                      global_step=tf.train.get_global_step())\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          loss=loss,\n",
    "                                          train_op=train_op)\n",
    "    \n",
    "    eval_metric_ops = {'accuracy': tf.metrics.accuracy(labels=labels,\n",
    "                                                       predictions=predictions['classes'])}\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      loss=losss,\n",
    "                                      eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T07:33:01.453456Z",
     "start_time": "2017-10-20T07:33:01.446450Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    cifar_classify = tf.estimator.Estimator(model_fn=part1_model,\n",
    "                                            model_dir='/tmp/part1')\n",
    "\n",
    "    tensors_log = {'probabilities': 'softmax'}\n",
    "    hook = tf.train.LoggingTensorHook(tensors=tensors_log, every_n_iter=50)\n",
    "\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(x={'x': x_tr},\n",
    "                                                        y=y_tr,\n",
    "                                                        batch_size=100,\n",
    "                                                        num_epochs=None,\n",
    "                                                        shuffle=True)\n",
    "    \n",
    "    cifar_classify.train(input_fn=train_input_fn,\n",
    "                         steps=20000,\n",
    "                         hooks=[hook]\n",
    "                        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T07:33:03.736963Z",
     "start_time": "2017-10-20T07:33:03.708938Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_save_checkpoints_secs': 600, '_model_dir': '/tmp/part1', '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_save_summary_steps': 100, '_tf_random_seed': 1}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-9d335f479abd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-47-e4bbb87d7b46>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(unused_argv)\u001b[0m\n\u001b[0;32m     14\u001b[0m     cifar_classify.train(input_fn=train_input_fn,\n\u001b[0;32m     15\u001b[0m                          \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                          \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                         )\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps)\u001b[0m\n\u001b[0;32m    239\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks)\u001b[0m\n\u001b[0;32m    626\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_and_assert_global_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m       features, labels = self._get_features_and_labels_from_input_fn(\n\u001b[1;32m--> 628\u001b[1;33m           input_fn, model_fn_lib.ModeKeys.TRAIN)\n\u001b[0m\u001b[0;32m    629\u001b[0m       estimator_spec = self._call_model_fn(features, labels,\n\u001b[0;32m    630\u001b[0m                                            model_fn_lib.ModeKeys.TRAIN)\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[1;34m(self, input_fn, mode)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_get_features_and_labels_from_input_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_input_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    583\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/cpu:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\inputs\\numpy_io.py\u001b[0m in \u001b[0;36minput_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    107\u001b[0m       \u001b[0mordered_dict_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munique_target_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mordered_dict_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m       shape_dict_of_x = {k: ordered_dict_x[k].shape\n\u001b[0;32m    111\u001b[0m                          for k in ordered_dict_x.keys()}\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\inputs\\numpy_io.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    107\u001b[0m       \u001b[0mordered_dict_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munique_target_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mordered_dict_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m       shape_dict_of_x = {k: ordered_dict_x[k].shape\n\u001b[0;32m    111\u001b[0m                          for k in ordered_dict_x.keys()}\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "main(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
